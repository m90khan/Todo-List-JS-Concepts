<h4><strong>Testing Overview</strong></h4><p>Testing is the process of finding errors. These errors can either be failures within the code, or they can be failures to meet requirements. If the app doesn't do what it was set out to do, then that is a problem. In testing, we work to make sure the program works for all requirements. </p><p><strong>Test Data - </strong>Inputs that are designed to test the system.</p><p><strong>Test Cases - </strong>Ways in which we operate the system with the given data. </p><p><strong>Oracle - </strong>The set of "good" outcomes. </p><h4><strong>Bugs</strong></h4><p>Bugs are in essence a deviation from expected behavior. For example, if you have a website, a potential bug might be that the website doesn't load. The code fatally breaks during loading, and results in a lack of the service. This is of course a deviation from the expected behavior of being able to access the website. </p><p>Another bug could be if the website logged you into someone else's account by mistake. The website in this scenario is still up and functioning. However, a deviation from expected behavior, only being able to access your own account, has happened. </p><p><strong>Failure - </strong>The event by which the code deviates from expected behavior. </p><p><strong>Error - </strong>The part of the code that leads to the failure. </p><p><strong>Fault - </strong>What the outcome actually was. </p><p>Testing can be used to show the presence of bugs, but never to ensure the absence of them. This is because the only way to ensure this, would be to test EVERY single possible test case. This could easily be in the billions to trillions for a typical system. This would be nearly impossible to do. The cost of testing alone could far exceed that of the entire rest of development times 10. </p><p>So what we do instead, is try to do the smallest set of test cases, to cover the greatest amount of ground. We use a mixture of different testing practices to accomplish this task. There will always be bugs, but we can actively remove most of them if we are smart. </p><h4><strong>Verification and Validation</strong></h4><p><strong>Verification - </strong>Are we building the thing right? Does the software work compared to the given specifications. </p><p><strong>Validation - </strong>Are we building the right thing? Does the software work compared to what the user/client needs? </p><p>A way of violating verification, would be if the program accesses the wrong database. In this situation, we are not building the system correctly. It is deviating from expected behavior. </p><p>A way of violating validation, would be if the program calculates car payments, instead of house payments. Our car payment calculator could be the most stable calculator in the world. However, we were supposed to build a house payment calculator. We are not building the right system. We are building a system which solves problems that it was not designed to solve. (And conversely doesn't solve problems that it was designed to solve.)</p><p>A more real world example of this would be if a company has a really specific way of collecting information and we designed a system which collects that information differently. All of the same information has been collected, but not in the right way. We aren't building the correct system. </p><p>Knowing, and testing both of these is important to delivering high quality software! </p><p>More Information: <a href="https://www.plutora.com/blog/verification-vs-validation" rel="noopener noreferrer" target="_blank">https://www.plutora.com/blog/verification-vs-validation</a></p>